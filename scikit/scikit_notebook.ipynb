{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains the following information: dict_keys(['data', 'target', 'frame', 'DESCR', 'feature_names', 'data_filename', 'target_filename', 'data_module'])\n",
      "Total number of samples inside data is '442' and there are '10' attributes to inference the 'target' column.\n",
      "\n",
      "Visualize the first few samples: \n",
      ".         age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
      "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
      "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
      "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
      "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
      "\n",
      "         s4        s5        s6  target  \n",
      "0 -0.002592  0.019907 -0.017646   151.0  \n",
      "1 -0.039493 -0.068332 -0.092204    75.0  \n",
      "2 -0.002592  0.002861 -0.025930   141.0  \n",
      "3  0.034309  0.022688 -0.009362   206.0  \n",
      "4 -0.002592 -0.031988 -0.046641   135.0  \n"
     ]
    }
   ],
   "source": [
    "# Load the diabetes dataset.\n",
    "ds = datasets.load_diabetes(as_frame=True)\n",
    "print(f\"The dataset contains the following information: {ds.keys()}\")\n",
    "\n",
    "# Check total number of rows and columns in the dataset.\n",
    "print(f\"Total number of samples inside data is '{ds.data.shape[0]}' and there are '{ds.data.shape[1]}' attributes to inference the '{ds.target.name}' column.\", end=\"\\n\\n\")\n",
    "\n",
    "# Obtain a brief look of the dataset.\n",
    "print(f\"Visualize the first few samples: \\n. {ds.frame.head()}\")\n",
    "\n",
    "# (Optional) See dataset description.\n",
    "# print(f\"Dataset contextual information:\\n {df.DESCR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (353, 10)\n",
      "Training target shape: (89, 10)\n",
      "Testing data shape: (353,)\n",
      "Testing target shape: (89,)\n"
     ]
    }
   ],
   "source": [
    "# Efectuate the partition of the dataset into training and testing data.\n",
    "x_train, y_train, x_test, y_test = train_test_split(ds.data, ds.target, test_size=0.2, random_state=42)\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Testing data shape: {x_test.shape}\")\n",
    "print(f\"Testing target shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 31718.0078 - mae: 159.8566 - val_loss: 22466.2773 - val_mae: 134.0236\n",
      "Epoch 2/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 33401.4258 - mae: 163.6038 - val_loss: 22435.6309 - val_mae: 133.9102\n",
      "Epoch 3/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 31673.0352 - mae: 158.7820 - val_loss: 22392.7598 - val_mae: 133.7521\n",
      "Epoch 4/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 33003.8320 - mae: 163.8864 - val_loss: 22330.8652 - val_mae: 133.5247\n",
      "Epoch 5/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 30749.4922 - mae: 157.4286 - val_loss: 22242.2012 - val_mae: 133.1999\n",
      "Epoch 6/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 31775.3984 - mae: 159.4357 - val_loss: 22118.0293 - val_mae: 132.7457\n",
      "Epoch 7/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 31363.7754 - mae: 158.6015 - val_loss: 21949.9023 - val_mae: 132.1301\n",
      "Epoch 8/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 32635.1270 - mae: 162.5044 - val_loss: 21727.6172 - val_mae: 131.3142\n",
      "Epoch 9/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 30585.3066 - mae: 155.5422 - val_loss: 21443.2422 - val_mae: 130.2646\n",
      "Epoch 10/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 29246.8730 - mae: 152.3179 - val_loss: 21088.7188 - val_mae: 128.9446\n",
      "Epoch 11/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 29408.3242 - mae: 151.2540 - val_loss: 20660.0586 - val_mae: 127.3309\n",
      "Epoch 12/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 30626.3320 - mae: 154.8615 - val_loss: 20137.9160 - val_mae: 125.3427\n",
      "Epoch 13/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 27981.2617 - mae: 148.6737 - val_loss: 19534.7168 - val_mae: 123.0016\n",
      "Epoch 14/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 28378.1172 - mae: 148.9114 - val_loss: 18836.4824 - val_mae: 120.2387\n",
      "Epoch 15/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 26049.0176 - mae: 141.3448 - val_loss: 18046.0312 - val_mae: 117.0377\n",
      "Epoch 16/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 25009.2266 - mae: 137.7833 - val_loss: 17168.7578 - val_mae: 113.3800\n",
      "Epoch 17/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 24581.9434 - mae: 136.9533 - val_loss: 16197.3945 - val_mae: 109.1954\n",
      "Epoch 18/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 22952.8828 - mae: 130.9615 - val_loss: 15151.3350 - val_mae: 104.5099\n",
      "Epoch 19/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 22559.2988 - mae: 126.3905 - val_loss: 14044.4678 - val_mae: 99.3136\n",
      "Epoch 20/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 20116.7324 - mae: 119.7977 - val_loss: 12885.6709 - val_mae: 93.5741\n",
      "Epoch 21/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 17739.1914 - mae: 110.4206 - val_loss: 11715.1475 - val_mae: 87.4410\n",
      "Epoch 22/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 18497.3203 - mae: 113.9335 - val_loss: 10522.7236 - val_mae: 81.2081\n",
      "Epoch 23/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 17012.8613 - mae: 108.9080 - val_loss: 9363.0762 - val_mae: 75.0576\n",
      "Epoch 24/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 13408.1875 - mae: 93.0781 - val_loss: 8270.5547 - val_mae: 69.0990\n",
      "Epoch 25/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 13352.9580 - mae: 91.2954 - val_loss: 7245.1895 - val_mae: 63.2408\n",
      "Epoch 26/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 10339.1816 - mae: 79.0882 - val_loss: 6336.6362 - val_mae: 58.3299\n",
      "Epoch 27/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 9066.2783 - mae: 74.4428 - val_loss: 5535.4023 - val_mae: 54.4485\n",
      "Epoch 28/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 8609.6826 - mae: 72.2984 - val_loss: 4872.7837 - val_mae: 51.5915\n",
      "Epoch 29/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 7344.2852 - mae: 67.0199 - val_loss: 4356.6362 - val_mae: 49.8539\n",
      "Epoch 30/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6938.9258 - mae: 65.4927 - val_loss: 3969.3872 - val_mae: 48.6089\n",
      "Epoch 31/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 5759.9263 - mae: 60.5172 - val_loss: 3719.9626 - val_mae: 48.0635\n",
      "Epoch 32/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 5787.1484 - mae: 62.3728 - val_loss: 3561.7014 - val_mae: 48.0892\n",
      "Epoch 33/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4403.4121 - mae: 54.2768 - val_loss: 3495.2756 - val_mae: 48.5687\n",
      "Epoch 34/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 4182.5112 - mae: 54.2839 - val_loss: 3479.3606 - val_mae: 49.2220\n",
      "Epoch 35/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 3842.9768 - mae: 51.3157 - val_loss: 3493.1416 - val_mae: 49.8485\n",
      "Epoch 36/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 4009.8450 - mae: 53.2951 - val_loss: 3532.9319 - val_mae: 50.6335\n",
      "Epoch 37/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 4166.3452 - mae: 54.9764 - val_loss: 3567.0366 - val_mae: 51.1447\n",
      "Epoch 38/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 3790.1489 - mae: 51.7946 - val_loss: 3586.0796 - val_mae: 51.4045\n",
      "Epoch 39/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 4019.4697 - mae: 52.9626 - val_loss: 3612.9902 - val_mae: 51.6970\n",
      "Epoch 40/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 3779.7458 - mae: 52.4344 - val_loss: 3615.1409 - val_mae: 51.7578\n",
      "Epoch 41/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3875.6130 - mae: 52.7730 - val_loss: 3622.1748 - val_mae: 51.8442\n",
      "Epoch 42/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3873.2058 - mae: 52.4065 - val_loss: 3630.1614 - val_mae: 51.9383\n",
      "Epoch 43/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3485.0562 - mae: 49.6829 - val_loss: 3621.1128 - val_mae: 51.8562\n",
      "Epoch 44/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3631.6184 - mae: 50.9748 - val_loss: 3606.6741 - val_mae: 51.7293\n",
      "Epoch 45/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 3697.5725 - mae: 51.6973 - val_loss: 3601.4868 - val_mae: 51.6798\n",
      "Epoch 46/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 3853.4048 - mae: 52.6743 - val_loss: 3593.0540 - val_mae: 51.5897\n",
      "Epoch 47/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3484.1941 - mae: 49.4333 - val_loss: 3589.8535 - val_mae: 51.5502\n",
      "Epoch 48/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3961.0813 - mae: 53.3625 - val_loss: 3569.8984 - val_mae: 51.3434\n",
      "Epoch 49/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3559.8977 - mae: 49.9125 - val_loss: 3565.1777 - val_mae: 51.2819\n",
      "Epoch 50/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3536.3398 - mae: 50.1263 - val_loss: 3571.8706 - val_mae: 51.3479\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3415.3491 - mae: 49.2103\n",
      "Test Loss: 3229.9091796875 \n",
      " Test MAE: 48.0242919921875\n"
     ]
    }
   ],
   "source": [
    "# Define the model using the Tensorflow library.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(x_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Regression output.\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])\n",
    "\n",
    "# Train the model.\n",
    "model.fit(x_train, x_test, epochs=50, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model.\n",
    "loss, mae = model.evaluate(y_train, y_test)\n",
    "print(f\"Test Loss: {loss} \\n Test MAE: {mae}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
